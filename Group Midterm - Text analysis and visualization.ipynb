{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Midterm - Text analysis and visualization\n",
    "\n",
    "The group members of this project are:\n",
    "* Devdhar Patel\n",
    "* Abhishek Singhal\n",
    "\n",
    "Notes for running this notebook: \n",
    "1. pip install wordcloud (visual c++ error may occur if it is not installed\n",
    "2. In case, the first step fails.\n",
    "   * Download the .whl file compatible with your Python version and your windows distribution (32bit or 64bit) from [here] http://www.lfd.uci.edu/~gohlke/pythonlibs/#wordcloud\n",
    "   * cd to the file path\n",
    "   * Run this command python -m pip install <filename>\n",
    "3. The tf-idf matrix may take time to compute, may run out of memory on machines with low memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "The data was scraped off the twitter website using [twitterscraper](https://github.com/taspinar/twitterscraper). We scraped 10,000 tweets with 'Big Data' in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"9e53251d-1c54-4942-b433-49d6c15c1765\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      var el = document.getElementById(\"9e53251d-1c54-4942-b433-49d6c15c1765\");\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"9e53251d-1c54-4942-b433-49d6c15c1765\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '9e53251d-1c54-4942-b433-49d6c15c1765' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.7.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.7.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.7.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.7.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      document.getElementById(\"9e53251d-1c54-4942-b433-49d6c15c1765\").textContent = \"BokehJS is loading...\";\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.7.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.7.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.7.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.7.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.7.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.7.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"9e53251d-1c54-4942-b433-49d6c15c1765\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullname</th>\n",
       "      <th>id</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>retweets</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Norbert Marek</td>\n",
       "      <td>918106625423982592</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Join us for a #Vertica \"Deep Dive\" in LA on Oc...</td>\n",
       "      <td>2017-10-11 13:30:51</td>\n",
       "      <td>NorbertMarek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jeff MAURY</td>\n",
       "      <td>918106691652210688</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3 Machine Learning Algorithms You Need to Know...</td>\n",
       "      <td>2017-10-11 13:31:07</td>\n",
       "      <td>jeffmaury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RACV</td>\n",
       "      <td>918106706651041792</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RT@ ipfconline1: 5 Actual #BigData Uses for Hu...</td>\n",
       "      <td>2017-10-11 13:31:11</td>\n",
       "      <td>racvfr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Optimize Intl</td>\n",
       "      <td>918106708060311552</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Turning #bigdata into business insights: The s...</td>\n",
       "      <td>2017-10-11 13:31:11</td>\n",
       "      <td>CIO_Success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nicholas O'Brien</td>\n",
       "      <td>918106739731521536</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The latest The Nicholas O'Brien Daily! http://...</td>\n",
       "      <td>2017-10-11 13:31:19</td>\n",
       "      <td>NickO_Brien</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           fullname                  id  likes  replies  retweets  \\\n",
       "0     Norbert Marek  918106625423982592      0        0         0   \n",
       "1        Jeff MAURY  918106691652210688      0        0         1   \n",
       "2              RACV  918106706651041792      0        0         0   \n",
       "3     Optimize Intl  918106708060311552      0        0         0   \n",
       "4  Nicholas O'Brien  918106739731521536      0        0         0   \n",
       "\n",
       "                                                text           timestamp  \\\n",
       "0  Join us for a #Vertica \"Deep Dive\" in LA on Oc... 2017-10-11 13:30:51   \n",
       "1  3 Machine Learning Algorithms You Need to Know... 2017-10-11 13:31:07   \n",
       "2  RT@ ipfconline1: 5 Actual #BigData Uses for Hu... 2017-10-11 13:31:11   \n",
       "3  Turning #bigdata into business insights: The s... 2017-10-11 13:31:11   \n",
       "4  The latest The Nicholas O'Brien Daily! http://... 2017-10-11 13:31:19   \n",
       "\n",
       "           user  \n",
       "0  NorbertMarek  \n",
       "1     jeffmaury  \n",
       "2        racvfr  \n",
       "3   CIO_Success  \n",
       "4   NickO_Brien  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import string\n",
    "import math\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, Select,LabelSet, HoverTool, BoxZoomTool,PanTool,WheelZoomTool,ResetTool\n",
    "from datetime import timedelta\n",
    "import datetime\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "from bokeh.application.handlers import FunctionHandler\n",
    "from bokeh.application import Application\n",
    "from bokeh.layouts import Row, widgetbox, Column\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer,CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from bokeh.palettes import Category10\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "\n",
    "df = pd.read_json('tweets.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizaions\n",
    "### 1. Word Cloud\n",
    "We create a word cloud based on the text of the tweets using a [word cloud generator](https://github.com/amueller/word_cloud). To install this word cloud generator, run:\n",
    "\n",
    "```pip install wordcloud```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Date: 2017-10-11\n",
      "Max Date: 2017-10-22\n"
     ]
    }
   ],
   "source": [
    "minDate = min(df['timestamp']).date()\n",
    "maxDate = max(df['timestamp']).date()\n",
    "print('Min Date:', minDate)\n",
    "print('Max Date:', maxDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dateList = []\n",
    "currentDate = minDate\n",
    "dateList.append(currentDate)\n",
    "while(currentDate < maxDate):\n",
    "    currentDate += timedelta(days=1)\n",
    "    dateList.append(currentDate)\n",
    "\n",
    "dateStrings = []\n",
    "for date in dateList:\n",
    "    dateStrings.append(date.strftime('%a,%d %b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type='text/javascript'>\n",
       "\n",
       "var target = document.getElementById('notebook-container');\n",
       "\n",
       "var observer = new MutationObserver(function(mutations) {\n",
       "\n",
       "   for (var i = 0; i < mutations.length; i++) {\n",
       "      for (var j=0; j < mutations[i].removedNodes.length; j++) {\n",
       "        for (var k=0; k < mutations[i].removedNodes[j].childNodes.length; k++)\n",
       "          var bokeh_selector = $(mutations[i].removedNodes[j].childNodes[k]).find(\".bokeh_class\");\n",
       "          if (bokeh_selector) {\n",
       "            if (bokeh_selector.length > 0) {\n",
       "               var destroyed_id = bokeh_selector[0].id;\n",
       "                \n",
       "var cmd = \"from bokeh import io; io._destroy_server('<%= destroyed_id %>')\";\n",
       "var command = _.template(cmd)({destroyed_id:destroyed_id});\n",
       "Jupyter.notebook.kernel.execute(command);\n",
       "\n",
       "            }\n",
       "          }\n",
       "      }\n",
       "   }\n",
       "});\n",
       "observer.observe(target, { childList: true, subtree:true });</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class='bokeh_class' id='f7f8208e39374123966c7337779a3f57'>\n",
       "<script\n",
       "    src=\"http://localhost:54089/autoload.js?bokeh-autoload-element=8f77e4bb-96e9-4e79-8100-b013791b0ba8&bokeh-app-path=/&bokeh-absolute-url=http://localhost:54089\"\n",
       "    id=\"8f77e4bb-96e9-4e79-8100-b013791b0ba8\"\n",
       "    data-bokeh-model-id=\"\"\n",
       "    data-bokeh-doc-id=\"\"\n",
       "></script></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The code below is commented to avoid generating a wordcloud every time the notebook is run.\n",
    "# wordcloud = WordCloud().generate(' '.join(list(df['text'])))\n",
    "# image = wordcloud.to_image()\n",
    "# image.save('wordcloud.bmp')\n",
    "\n",
    "# for index,date in enumerate(dateList):\n",
    "#     start = datetime.datetime(date.year,date.month,date.day,0,0,0)\n",
    "#     end = datetime.datetime(date.year,date.month,date.day, 23,59,59)\n",
    "#     tweets = df.loc[(start <= df['timestamp']) & (end >= df['timestamp'])]\n",
    "#     wordcloud = WordCloud().generate(' '.join(list(df['text'])))\n",
    "#     image = wordcloud.to_image()\n",
    "#     image.save('wordcloud'+str(chr(97 + index))+'.bmp')\n",
    "\n",
    "def modify_doc1(doc):\n",
    "       \n",
    "    def update(attr, old, new):\n",
    "        layout.children[1] = create_figure()\n",
    "        \n",
    "    \n",
    "    options = list(dateStrings)\n",
    "    options.append('All Days')\n",
    "    dateSelect = Select(title=\"Date:\", options=options, value=options[-1])\n",
    "    dateSelect.on_change('value', update)\n",
    "    \n",
    "    \n",
    "    def create_figure():\n",
    "        p = figure(x_range=(0,2), y_range=(0,2), width=600, height=300, tools=\"pan, wheel_zoom, reset\")\n",
    "        p.xgrid.grid_line_color = None\n",
    "        p.ygrid.grid_line_color = None\n",
    "        p.axis.visible = False\n",
    "        if dateSelect.value == 'All Days':\n",
    "            url = 'wordcloud.bmp'\n",
    "        else:\n",
    "            url = 'wordcloud' + str(chr(97+options.index(dateSelect.value))) + '.bmp'\n",
    "        p.image_url(url=[url], x=0, y=2,w=2, h=2 )\n",
    "        return p\n",
    "    p = create_figure()\n",
    "    layout = Column(dateSelect, p)\n",
    "    doc.add_root(layout)\n",
    "    \n",
    "\n",
    "handler1 = FunctionHandler(modify_doc1)\n",
    "app1 = Application(handler1)\n",
    "app1.create_document()\n",
    "show(app1, notebook_url=\"localhost:8888\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** As we can see from the generated image, the words big data nad twitter are some of the most common words which is to be expected. Howver, some one the other common words are associated with links: https, goo, gl, ly, etc. Other notable words are: machine learning, paper, web and status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tweet frequencies of different hashtags\n",
    "Next we will create a tweet frequency graph over time for the top 20 different hashtags. To do this, we will first need to extract all the hashtags from the text of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtagsColumn = []\n",
    "hashtagDictionary = {}\n",
    "for index,row in df.iterrows():\n",
    "    words = row['text'].split()\n",
    "    hashtags = []\n",
    "    for word in words:\n",
    "        if(word[0] == '#'):\n",
    "            hashtags.append(word)\n",
    "            if word in hashtagDictionary:\n",
    "                hashtagDictionary[word] += 1\n",
    "            else:\n",
    "                hashtagDictionary[word] = 1\n",
    "            \n",
    "    hashtagsColumn.append(hashtags)\n",
    "\n",
    "df['hashtags'] = hashtagsColumn\n",
    "hashtagSeries = pd.Series(hashtagDictionary)\n",
    "hashtagSeries = hashtagSeries.sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='bokeh_class' id='3b26b296491c49129b19d0499347c04b'>\n",
       "<script\n",
       "    src=\"http://localhost:54094/autoload.js?bokeh-autoload-element=6e9a9e69-b667-4a1c-bc5e-9616bb13434a&bokeh-app-path=/&bokeh-absolute-url=http://localhost:54094\"\n",
       "    id=\"6e9a9e69-b667-4a1c-bc5e-9616bb13434a\"\n",
       "    data-bokeh-model-id=\"\"\n",
       "    data-bokeh-doc-id=\"\"\n",
       "></script></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dateStrings = []\n",
    "for date in dateList:\n",
    "    dateStrings.append(date.strftime('%a,%d %b'))\n",
    "        \n",
    "\n",
    "def modify_doc(doc):\n",
    "       \n",
    "    def update(attr, old, new):\n",
    "        layout.children[1] = create_figure()\n",
    "    \n",
    "    options = list(hashtagSeries.keys())[0:20]\n",
    "    options.append('All Tweets')\n",
    "    hashtagSelect = Select(title=\"Hashtag:\", options=options, value=hashtagSeries.keys()[0])\n",
    "    hashtagSelect.on_change('value', update)\n",
    "    \n",
    "    \n",
    "    def create_figure():\n",
    "        hashTagCount = []\n",
    "        for date in dateList:\n",
    "            start = datetime.datetime(date.year,date.month,date.day,0,0,0)\n",
    "            end = datetime.datetime(date.year,date.month,date.day, 23,59,59)\n",
    "            tweets = df.loc[(start <= df['timestamp']) & (end >= df['timestamp'])]\n",
    "            count = 0\n",
    "            if(hashtagSelect.value == 'All Tweets'):\n",
    "                count += len(tweets)\n",
    "            else:\n",
    "                for row in tweets.iterrows():\n",
    "                    tags = row[1]['hashtags']\n",
    "                    if hashtagSelect.value in tags:\n",
    "                        count += 1\n",
    "            hashTagCount.append(count)\n",
    "        hover = HoverTool(tooltips=[\n",
    "            (\"count\", \"@y\"),\n",
    "        ])\n",
    "        source = ColumnDataSource(data = {'x': dateStrings, 'y':hashTagCount})\n",
    "        p = figure(width=800, height=250, x_range=dateStrings, tools=[hover], title='Count of ' + hashtagSelect.value)\n",
    "        p.vbar('x',top='y',width=0.5,source=source, bottom=0)\n",
    "        return p\n",
    "    \n",
    "    p = create_figure()\n",
    "    layout = Column(hashtagSelect, p)\n",
    "    doc.add_root(layout)\n",
    "\n",
    "handler = FunctionHandler(modify_doc)\n",
    "app = Application(handler)\n",
    "show(app, notebook_url=\"localhost:8888\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** As we can see, a lot of tweets related to big data were tweeted on 11 Oct 2017. This could be due to a lot of big data event happening on that particular day. Here are a few of the events that were hosted on 11 Oct:\n",
    "1. [AWS Tech Talk - Big Data by Chris Widmann](https://www.eventbrite.com/e/aws-tech-talk-big-data-tickets-37541684188#)\n",
    "2. [11-12 October 2017: 2nd BMBF Big Data All Hands Meeting and 2nd Smart Data Innovation Conference](http://www.bdva.eu/?q=node/838)\n",
    "3. [Predictive Analytics World London, 11-12 October, 2017](https://predictiveanalyticsworld.co.uk/)\n",
    "4. [DEEP LEARNING SUMMIT MONTREAL](https://www.re-work.co/events/deep-learning-summit-montreal-canada-track1-2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Day-wise word count (trending words of the day)\n",
    "We will now create a graph to look at the trending words of each day. To create this graph, we will ignore common words that are popular across all days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='bokeh_class' id='56a691ee2d9d4904b3e611aeebd99244'>\n",
       "<script\n",
       "    src=\"http://localhost:54108/autoload.js?bokeh-autoload-element=0f1c1a9e-663a-43b5-940c-5c6a0071dc4e&bokeh-app-path=/&bokeh-absolute-url=http://localhost:54108\"\n",
       "    id=\"0f1c1a9e-663a-43b5-940c-5c6a0071dc4e\"\n",
       "    data-bokeh-model-id=\"\"\n",
       "    data-bokeh-doc-id=\"\"\n",
       "></script></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "garbageTokens = ['¿','...','…','’','RT','–','el','la','via','en','que',\"it's\",'es'] # we remove RT because it is associated with re-tweet\n",
    "top10EachDay = []\n",
    "for date in dateList:\n",
    "    start = datetime.datetime(date.year,date.month,date.day,0,0,0)\n",
    "    end = datetime.datetime(date.year,date.month,date.day, 23,59,59)\n",
    "    tweets = df.loc[(start <= df['timestamp']) & (end >= df['timestamp'])]\n",
    "    text = ' '.join(list(tweets['text']))\n",
    "    tokens = TweetTokenizer(preserve_case=False).tokenize(text)\n",
    "    stop = stopwords.words('english') + list(string.punctuation) + garbageTokens\n",
    "    filteredTokens = [w for w in tokens if not w in stop]\n",
    "    counts = Counter(filteredTokens)\n",
    "    counts = pd.Series(counts)\n",
    "    counts = counts.sort_values(ascending=False)\n",
    "    top10EachDay.extend(list(counts.keys())[0:10])\n",
    "\n",
    "top10EachDay = set(top10EachDay)\n",
    "    \n",
    "def modify_doc2(doc):\n",
    "       \n",
    "    def update(attr, old, new):\n",
    "        layout.children[1] = create_figure()\n",
    "        \n",
    "    \n",
    "    options = list(dateStrings)\n",
    "    options.append('All Days')\n",
    "    dateSelect = Select(title=\"Date:\", options=options, value=options[0])\n",
    "    dateSelect.on_change('value', update)\n",
    "    \n",
    "    \n",
    "    def create_figure():\n",
    "        dateIndex = options.index(dateSelect.value)\n",
    "        if dateIndex == (len(options) - 1):\n",
    "            tweets = df\n",
    "        else:\n",
    "            date = dateList[dateIndex]\n",
    "            start = datetime.datetime(date.year,date.month,date.day,0,0,0)\n",
    "            end = datetime.datetime(date.year,date.month,date.day, 23,59,59)\n",
    "            tweets = df.loc[(start <= df['timestamp']) & (end >= df['timestamp'])]\n",
    "        \n",
    "        text = ' '.join(list(tweets['text']))\n",
    "        tokens = TweetTokenizer(preserve_case=False).tokenize(text)\n",
    "        stop = stopwords.words('english') + list(string.punctuation) + garbageTokens\n",
    "        filteredTokens = [w for w in tokens if not w in stop]\n",
    "        counts = Counter(filteredTokens)\n",
    "        counts = pd.Series(counts)\n",
    "        counts = counts.sort_values(ascending=False)\n",
    "        hover = HoverTool(tooltips=[\n",
    "            (\"count\", \"@y\"),\n",
    "        ])\n",
    "        x = []\n",
    "        y = []\n",
    "        count = 0\n",
    "        if dateIndex != (len(options) - 1):\n",
    "            for item in counts.iteritems():\n",
    "                if (item[0] not in top10EachDay):\n",
    "                    x.append(item[0])\n",
    "                    y.append(item[1])\n",
    "                    count += 1\n",
    "                if(count == 10):\n",
    "                    break\n",
    "        else:\n",
    "            x = list(counts.keys())[0:10]\n",
    "            y = list(counts)[0:10]\n",
    "        source = ColumnDataSource(data = {'x': x, 'y':y})\n",
    "        p = figure(width=800, height=250, x_range=x, tools=[hover], title='Trending words for ' + dateSelect.value)\n",
    "        p.vbar('x',top='y',width=0.5,source=source, bottom=0)\n",
    "#         p.xaxis.major_label_orientation = -math.pi/3\n",
    "        return p\n",
    "    \n",
    "    p = create_figure()\n",
    "    layout = Column(dateSelect, p)\n",
    "    doc.add_root(layout)\n",
    "\n",
    "handler2 = FunctionHandler(modify_doc2)\n",
    "app2 = Application(handler2)\n",
    "app2.create_document()\n",
    "show(app2, notebook_url=\"localhost:8888\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** This graph provides some really insightful data. Here are some of the observations that we think are interesting:\n",
    "1. On 15th october, the forbes website published this [article](http://ift.tt/2hIKAiC) Therefore, the url and the words age, app,latest, and rethink are trending on 15th Oct.\n",
    "2. On 16th october, the Smart Cities Summit 2017 was hosted which results in #smartcities trending.\n",
    "2. On 17th october, CNBC published this [story](https://www.cnbc.com/2017/10/17/thoughtspot-ceo-problem-with-big-data-isnt-visualization-its-scale.html). The headline was: A.I. company CEO: Big data is not a visualization problem, it's a human scale problem. This is reflected on the trending words that day being: human, ceo and company\n",
    "4. On 19th october, TechCrunch published this [article](https://techcrunch.com/2017/10/19/data-is-the-name-of-the-game-as-intel-capital-puts-60m-in-15-startups-566m-in-2017-overall/) with the title: Data is the name of the game, as Intel Capital puts $60M in 15 startups, $566M in 2017 overall written by Ingrid Lunden. This resulted in a bunch of trending words: 60m, startups, 566m, invests, intel, total, ingrid and investments.\n",
    "5. On 22nd october, The Wall Street Journal published this (article)[https://www.wsj.com/articles/how-facebooks-master-algorithm-powers-the-social-network-1508673600] with the title: How Facebook’s Master Algorithm Powers the Social Network. This resulted in a bunch of trending words including facebook, dictators and evil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "Next we will run clustering algorithms on frequency–inverse document frequency for words and tags in each tweet.\n",
    "\n",
    "**Tokenize**\n",
    "1. First, we tokenize the given tweet text using NLTK library.\n",
    "2. We the convert the text data to numerical equivalent. This was done in two different ways for two different texts.\n",
    "   * For the tweet text, we created tf-idf matrix using tf-idf vectorizer. Note that, we can use unigram, bigram and in general n-grams as parameter. This will give different features everytime.\n",
    "   * For hashtags, we created Count matrix. This will give features based on term frequency, doesnot involve inverse document frequency. CountVectorizer func. was used.\n",
    "3. We calculated cosine similarity distance (to be later used for dimensionality reduction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10019, 28352)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-6bd9c9de7021>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m#calcluate distance between different tweets based on tf-idf matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m#will be used for clustering and visualization later on\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mdistWords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\abhishek\\Anaconda\\envs\\py36\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m    916\u001b[0m         \u001b[0mY_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m     \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_normalized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_normalized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdense_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\abhishek\\Anaconda\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \"\"\"\n\u001b[0;32m    183\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdense_output\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"toarray\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\abhishek\\Anaconda\\envs\\py36\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dimension mismatch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_sparse_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m         \u001b[1;31m# If it's a list or whatever, treat it like a matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\abhishek\\Anaconda\\envs\\py36\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m_mul_sparse_matrix\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    538\u001b[0m                                     maxval=nnz)\n\u001b[0;32m    539\u001b[0m         \u001b[0mindptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 540\u001b[1;33m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnnz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    541\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnnz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#take text in twitter to 'cluster on' from dataframe\n",
    "textDF = df['text']\n",
    "tokenizer = TweetTokenizer(preserve_case=False)\n",
    "#tokenize given text\n",
    "def tokenize(text):\n",
    "    #for userComments in commentsDf:\n",
    "    textTokens = tokenizer.tokenize(text)\n",
    "    stop = stopwords.words('english') + list(string.punctuation) + garbageTokens\n",
    "    filteredTokens = [w for w in textTokens if not w in stop]\n",
    "    #filter out punctuations and numeric tokens\n",
    "    return filteredTokens\n",
    "\n",
    "\n",
    "#making Term Frequency-Inverse document Frequency matrix model\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8,\n",
    "                                 min_df=0, stop_words='english',\n",
    "                                 use_idf=True,tokenizer=tokenize, ngram_range=(1,1))\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(textDF)\n",
    "print (tfidf_matrix.shape)\n",
    "wordFeatures = tfidf_vectorizer.get_feature_names()   #this may be different from totalVocabTokens\n",
    "\n",
    "#calcluate distance between different tweets based on tf-idf matrix\n",
    "#will be used for clustering and visualization later on\n",
    "distWords = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code to calculate frequency of each hashtag\n",
    "#will be later used to cluster on hashtags\n",
    "def join(x):\n",
    "    return \" \".join(x['hashtags'])\n",
    "hashtagsDf = df.apply(join, axis=1)\n",
    "countHashtagsVectorizer = CountVectorizer(stop_words='english',tokenizer=tokenize,ngram_range=(1,1))\n",
    "countHasgtagsMatrix = countHashtagsVectorizer.fit_transform(hashtagsDf)\n",
    "\n",
    "#calcluate distance between different tweets based on tf-idf matrix\n",
    "#will be used for clustering and visualization later on\n",
    "distHashtags = 1 - cosine_similarity(countHasgtagsMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Clustering uses tf-idf or count matrix. **\n",
    "1. For text tweets, clustering KMeans is applied\n",
    "2. For hashtags clustering, DBscan is used.\n",
    "We can apply any clustering algo on any matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clusteringAlgo(matrix,para):\n",
    "        clf = KMeans(n_clusters=para)\n",
    "        clf.fit(matrix)\n",
    "        \n",
    "        order_centroids = clf.cluster_centers_.argsort()[:,::-1]\n",
    "        topFeaturesInEachCluster=[]\n",
    "        for centres in order_centroids:\n",
    "            featuredWords=[]\n",
    "            for words in centres[:5]:\n",
    "                featuredWords.append(wordFeatures[words])\n",
    "            topFeaturesInEachCluster.append(featuredWords)\n",
    "        clusters = clf.labels_.tolist()\n",
    "        return clusters,topFeaturesInEachCluster\n",
    "\n",
    "\n",
    "\n",
    "clusterWords,topFeaturesWordsInEachCluster = clusteringAlgo(tfidf_matrix,5)\n",
    "clusterHashtags, topHashtagFeatures = clusteringAlgo(countHasgtagsMatrix,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The plot is shown is reduced dimension space for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimensionality reduction for the TF-IDF matrix (or df equally)\n",
    "def dimensionReduction(dist):\n",
    "\n",
    "    # convert two components as we're plotting points in a two-dimensional plane\n",
    "\n",
    "    # we will also specify `random_state` so the plot is reproducible.\n",
    "    mds = TruncatedSVD(n_components=2)\n",
    "\n",
    "    #toarray() converts sparse array to dense numpy array\n",
    "    pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "    return pos\n",
    "    #store the dimensions in xs, ys\n",
    "    #xs, ys = pos[:, 0], pos[:, 1]\n",
    "    \n",
    "hovera = HoverTool(tooltips=[\n",
    "            (\"Tweet\", \"@text\"),\n",
    "        ])\n",
    "a=figure(plot_height=500, plot_width=900, title='clustering groups based on Tweet Text',tools=[BoxZoomTool(),PanTool(),WheelZoomTool(),ResetTool(),hovera])\n",
    "\n",
    "dimensionReduced = dimensionReduction(distWords)\n",
    "x = dimensionReduced[:, 0]\n",
    "y = dimensionReduced[:, 1]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,5):\n",
    "    \n",
    "    filteredX = []\n",
    "    filteredY = []\n",
    "    text = []\n",
    "    for index, cluster in enumerate(clusterWords):\n",
    "        if(cluster == i):\n",
    "            filteredX.append(x[index])\n",
    "            filteredY.append(y[index])\n",
    "            text.append(textDF[index])\n",
    "    source = ColumnDataSource(data={'x':filteredX,'y':filteredY, 'text':text})\n",
    "    a.circle('x','y',source=source,color=Category10[5][i], size=10, alpha=0.2,legend='Top Features:' \n",
    "             + \",\".join(topFeaturesWordsInEachCluster[i]))\n",
    "\n",
    "a.legend.location = \"top_left\"\n",
    "a.legend.click_policy=\"hide\"\n",
    "show(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoverb = HoverTool(tooltips=[\n",
    "            (\"Hashtags\", \"@text\"),\n",
    "        ])\n",
    "\n",
    "b=figure( plot_height=500, plot_width=900, title='clustering groups based on Hashtags', tools=[BoxZoomTool(),PanTool(),WheelZoomTool(),ResetTool(),hoverb])\n",
    "\n",
    "dimensionReduced = dimensionReduction(distHashtags)\n",
    "x = dimensionReduced[:, 0]\n",
    "y = dimensionReduced[:, 1]\n",
    "\n",
    "#user are labels\n",
    "for i in range(0,2):\n",
    "    \n",
    "    filteredX = []\n",
    "    filteredY = []\n",
    "    text = []\n",
    "    for index, cluster in enumerate(clusterHashtags):\n",
    "        if(cluster == i):\n",
    "            filteredX.append(x[index])\n",
    "            filteredY.append(y[index])\n",
    "            text.append(hashtagsDf[index])\n",
    "    source = ColumnDataSource(data={'x':filteredX,'y':filteredY, 'text':text})\n",
    "    b.circle('x','y',source=source,color=Category10[5][i], size=10, alpha=0.2,legend='Top Features:' \n",
    "             + \",\".join(topHashtagFeatures[i]))\n",
    "\n",
    "b.legend.location = \"top_right\"\n",
    "b.legend.click_policy=\"hide\"\n",
    "show(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
